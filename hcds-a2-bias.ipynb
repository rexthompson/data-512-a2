{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias on Wikipedia\n",
    "\n",
    "For this assignment (https://wiki.communitydata.cc/HCDS_(Fall_2017)/Assignments#A2:_Bias_in_data), your job is to analyze what the nature of political articles on Wikipedia - both their existence, and their quality - can tell us about bias in Wikipedia's content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "data_dir = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingest\n",
    "\n",
    "First step is to get the data. We need population data and article data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population Data\n",
    "\n",
    "Download population data from Population Reference Bureau here:  \n",
    "http://www.prb.org/DataFinder/Topic/Rankings.aspx?ind=14\n",
    "\n",
    "This data is saved in `./data/raw/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>Location Type</th>\n",
       "      <th>TimeFrame</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Data</th>\n",
       "      <th>Footnotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Country</td>\n",
       "      <td>Mid-2015</td>\n",
       "      <td>Number</td>\n",
       "      <td>32247000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albania</td>\n",
       "      <td>Country</td>\n",
       "      <td>Mid-2015</td>\n",
       "      <td>Number</td>\n",
       "      <td>2892000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>Country</td>\n",
       "      <td>Mid-2015</td>\n",
       "      <td>Number</td>\n",
       "      <td>39948000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Andorra</td>\n",
       "      <td>Country</td>\n",
       "      <td>Mid-2015</td>\n",
       "      <td>Number</td>\n",
       "      <td>78000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Location Location Type TimeFrame Data Type      Data  Footnotes\n",
       "0  Afghanistan       Country  Mid-2015    Number  32247000        NaN\n",
       "1      Albania       Country  Mid-2015    Number   2892000        NaN\n",
       "2      Algeria       Country  Mid-2015    Number  39948000        NaN\n",
       "3      Andorra       Country  Mid-2015    Number     78000        NaN"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From local data\n",
    "filename = data_dir + 'raw/Population Mid-2015.csv'\n",
    "population_data = pd.read_csv(filename, skiprows=2, thousands=',')\n",
    "population_data.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # From website\n",
    "# filename = 'http://www.prb.org/RawData.axd?ind=14&fmt=14&tf=76&loc=34235%2c249%2c250%2c251%2c252%2c253%2c254%2c34227%2c255%2c257%2c258%2c259%2c260%2c261%2c262%2c263%2c264%2c265%2c266%2c267%2c268%2c269%2c270%2c271%2c272%2c274%2c275%2c276%2c277%2c278%2c279%2c280%2c281%2c282%2c283%2c284%2c285%2c286%2c287%2c288%2c289%2c290%2c291%2c292%2c294%2c295%2c296%2c297%2c298%2c299%2c300%2c301%2c302%2c304%2c305%2c306%2c307%2c308%2c311%2c312%2c315%2c316%2c317%2c318%2c319%2c320%2c321%2c322%2c324%2c325%2c326%2c327%2c328%2c34234%2c329%2c330%2c331%2c332%2c333%2c334%2c336%2c337%2c338%2c339%2c340%2c342%2c343%2c344%2c345%2c346%2c347%2c348%2c349%2c350%2c351%2c352%2c353%2c354%2c358%2c359%2c360%2c361%2c362%2c363%2c364%2c365%2c366%2c367%2c368%2c369%2c370%2c371%2c372%2c373%2c374%2c375%2c377%2c378%2c379%2c380%2c381%2c382%2c383%2c384%2c385%2c386%2c387%2c388%2c389%2c390%2c392%2c393%2c394%2c395%2c396%2c397%2c398%2c399%2c400%2c401%2c402%2c404%2c405%2c406%2c407%2c408%2c409%2c410%2c411%2c415%2c416%2c417%2c418%2c419%2c420%2c421%2c422%2c423%2c424%2c425%2c427%2c428%2c429%2c430%2c431%2c432%2c433%2c434%2c435%2c437%2c438%2c439%2c440%2c441%2c442%2c443%2c444%2c445%2c446%2c448%2c449%2c450%2c451%2c452%2c453%2c454%2c455%2c456%2c457%2c458%2c459%2c460%2c461%2c462%2c464%2c465%2c466%2c467%2c468%2c469%2c470%2c471%2c472%2c473%2c474%2c475%2c476%2c477%2c478%2c479%2c480'\n",
    "# population_data = pd.read_csv(filename, skiprows=2, thousands=',')\n",
    "# population_data.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: consider using Gary Gregg's map to update some of the country names\n",
    "\n",
    "country_map = {\n",
    "   \"East Timorese\" : \"Timor-Leste\",\n",
    "   \"Hondura\" : \"Honduras\",\n",
    "   \"Rhodesian\" : \"Zimbabwe\",\n",
    "   \"Salvadoran\" : \"El Salvador\",\n",
    "   \"Samoan\" : \"Samoa\",\n",
    "   \"São Tomé and Príncipe\" : \"Sao Tome and Principe\",\n",
    "   # \"Somaliland\" : \"Somalia\",  # Oliver says this one is not correct\n",
    "   \"South African Republic\" : \"South Africa\",\n",
    "   \"South Korean\" : \"Korea, South\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so we have the population data by country, both direct from online and from local data. Good.\n",
    "\n",
    "Now let's see about getting the article data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Politician/Article Data\n",
    "\n",
    "You'll find the wikipedia politician article dataset on Figshare here:  \n",
    "https://figshare.com/articles/Untitled_Item/5513449\n",
    "\n",
    "If you want to do this yourself, you'll need to go to the link above, read through the documentation for this repository, then download and unzip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From website: TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>country</th>\n",
       "      <th>rev_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Template:ZambiaProvincialMinisters</td>\n",
       "      <td>Zambia</td>\n",
       "      <td>235107991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bir I of Kanem</td>\n",
       "      <td>Chad</td>\n",
       "      <td>355319463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Template:Zimbabwe-politician-stub</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>391862046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Template:Uganda-politician-stub</td>\n",
       "      <td>Uganda</td>\n",
       "      <td>391862070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 page   country     rev_id\n",
       "0  Template:ZambiaProvincialMinisters    Zambia  235107991\n",
       "1                      Bir I of Kanem      Chad  355319463\n",
       "2   Template:Zimbabwe-politician-stub  Zimbabwe  391862046\n",
       "3     Template:Uganda-politician-stub    Uganda  391862070"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From local data\n",
    "filename = data_dir + 'raw/page_data.csv'\n",
    "page_data = pd.read_csv(filename)\n",
    "page_data.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article Scores from ORES\n",
    "\n",
    "Now that we have our article data, we can get scores for each article using the ORES API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation for the ORES API can be found here:  \n",
    "https://ores.wikimedia.org/v3/#!/scoring/get_v3_scores_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "endpoint = 'https://ores.wikimedia.org/v3/scores/{project}/?models={model}&revids={revids}'\n",
    "headers = {'User-Agent' : 'https://github.com/rexthompson', 'From' : 'rext@uw.edu'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "revids_all = list(page_data['rev_id'])\n",
    "\n",
    "# TEMP FOR TESTING\n",
    "revids_all = revids_all[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up empty dataframe to hold results for each article\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# loop 100 entries at a time\n",
    "idx_start = 0\n",
    "idx_end = 100\n",
    "while idx_start < len(revids_all):\n",
    "    \n",
    "    # retrieve and concatenate subset of revids\n",
    "    revids = revids_all[idx_start:idx_end]\n",
    "    revids = '|'.join(str(x) for x in revids)\n",
    "    \n",
    "    # pull article data from API\n",
    "    params = {'project' : 'enwiki',\n",
    "              'revids' : revids,\n",
    "              'model' : 'wp10'\n",
    "          }\n",
    "    \n",
    "    api_call = requests.get(endpoint.format(**params), headers)\n",
    "    response = api_call.json()\n",
    "    \n",
    "    for revid in response['enwiki']['scores']:\n",
    "        try:\n",
    "            # temp_dict = response['enwiki']['scores'][revid]['wp10']['score']['probability']\n",
    "            # rating = max(temp_dict, key=temp_dict.get)\n",
    "            rating = response['enwiki']['scores'][revid]['wp10']['score']['prediction']\n",
    "        except:\n",
    "            rating = np.nan\n",
    "        df = df.append({'revid':revid, 'rating':rating}, ignore_index=True)\n",
    "    \n",
    "    # NOTE -- ratings do not return in the same order as they were passed to the API!!!\n",
    "    \n",
    "    # update indexes\n",
    "    idx_start += 100\n",
    "    idx_end = min(idx_start+100, len(revids_all))\n",
    "    \n",
    "# revid, temp_dict, rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this data looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so now we have a nice table of ratings for each article. Let's merge this back with the original article data. First we convert the revids to ints since they are currently strings. We need them to be the same as page_data which has this as an int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['revid'] = pd.to_numeric(df['revid'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we merge, and drop the redundant column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rating_data = page_data.merge(df, left_on='rev_id', right_on='revid').drop('revid', 1)\n",
    "rating_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "population_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we'll merge both datasets on the Country column (or the \"Location\" column for the population data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_df = population_data.merge(rating_data, left_on='Location', right_on='country')\n",
    "merged_df_new = pd.DataFrame({'country':merged_df['Location'],\n",
    "                              'population':merged_df['Data'],\n",
    "                              'article_name':merged_df['page'],\n",
    "                              'revision_id':merged_df['rev_id'],\n",
    "                              'article_quality':merged_df['rating']})\n",
    "\n",
    "#convert population to int\n",
    "pd.to_numeric(merged_df_new['population'])\n",
    "\n",
    "#reorder columns\n",
    "merged_df_new = merged_df_new[['country',\n",
    "                               'population',\n",
    "                               'arrticle_name',\n",
    "                               'revision_id',\n",
    "                               'article_quality']]\n",
    "merged_df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save this data to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set filename for combined data CSV\n",
    "filename = data_dir + 'merged_data.csv'\n",
    "\n",
    "# check if file already exists; load if so, create if not\n",
    "if os.path.isfile(filename):\n",
    "    merged_df_new = pd.read_csv(filename)\n",
    "    print('loaded CSV data from ' + filename)\n",
    "else:\n",
    "    merged_df_new.to_csv(filename, index=False)\n",
    "    print('saved CSV data to ' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we have a good database with ratings for each article. We can now group by country to count how many of each type there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(merged_df_new.groupby(['country']).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The API can only handle so many requests at a time, so we'll go 100 at a time.\n",
    "\n",
    "\n",
    "# loop over \n",
    "params = {'project' : 'enwiki',\n",
    "          'revids' : '797882322',\n",
    "          'model' : 'wp10'\n",
    "          }\n",
    "\n",
    "api_call = requests.get(endpoint.format(**params), headers)\n",
    "response = api_call.json()\n",
    "response\n",
    "#print(json.dumps(response, indent=4, sort_keys=True))\n",
    "\n",
    "for revid in response['enwiki']['scores']:\n",
    "    print(revid)\n",
    "    temp_dict = response['enwiki']['scores'][revid]['wp10']['score']['probability']\n",
    "    rating = max(temp_dict, key=temp_dict.get)\n",
    "    print(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the other data is just a matter of reading CSV files in! (and for the R programmers - we'll have an R example up as soon as the Hub supports the language)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## getting the data from the CSV files\n",
    "import csv\n",
    "\n",
    "data = []\n",
    "with open('page_data.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        data.append([row[0],row[1],row[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(data[782])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note from Andrew Enfield on Slack on 10/25/17:\n",
    "\n",
    "FYI that I was able to get scores for all articles in just two minutes with the https://ores.wikimedia.org/v3/#!/scoring/get_v3_scores_context API. I retrieved the scores in chunks of 140 articles at a time/per call - when I experimented, 140 always worked but 145 or 150 gave me errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD STUFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias on Wikipedia\n",
    "\n",
    "For this assignment (https://wiki.communitydata.cc/HCDS_(Fall_2017)/Assignments#A2:_Bias_in_data), your job is to analyze what the nature of political articles on Wikipedia - both their existence, and their quality - can tell us about bias in Wikipedia's content.\n",
    "\n",
    "# Making ORES requests\n",
    "\n",
    "Below is an example of how to make requests through the ORES system in Python to find out the current quality of an article. Specifically, this is a function designed to make a request with *multiple* revision IDs. You can take this function, split your revision IDs up into chunks of 50 or 100 to avoid hitting limits in ORES, pass each chunk through this function, and then stitch the whole set together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "headers = {'User-Agent' : 'https://github.com/your_github_username', 'From' : 'your_uw_email@uw.edu'}\n",
    "\n",
    "def get_ores_data(revision_ids, headers):\n",
    "    \n",
    "    # Define the endpoint\n",
    "    endpoint = 'https://ores.wikimedia.org/v3/scores/{project}/?models={model}&revids={revids}'\n",
    "    \n",
    "    # Specify the parameters - smushing all the revision IDs together separated by | marks.\n",
    "    # Yes, 'smush' is a technical term, trust me I'm a scientist.\n",
    "    # What do you mean \"but people trusting scientists regularly goes horribly wrong\" who taught you tha- oh.  \n",
    "    params = {'project' : 'enwiki',\n",
    "              'model'   : 'wp10',\n",
    "              'revids'  : '|'.join(str(x) for x in revision_ids)\n",
    "              }\n",
    "    api_call = requests.get(endpoint.format(**params))\n",
    "    response = api_call.json()\n",
    "    print(json.dumps(response, indent=4, sort_keys=True))\n",
    "\n",
    "\n",
    "# So if we grab some example revision IDs and turn them into a list and then call get_ores_data...\n",
    "example_ids = [783381498, 807355596, 757539710]\n",
    "get_ores_data(example_ids, headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the other data is just a matter of reading CSV files in! And if you're an R programmer wondering where the R example is - check the other file in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## getting the data from the CSV files\n",
    "import csv\n",
    "\n",
    "data = []\n",
    "with open('./data/raw/page_data.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        data.append([row[0],row[1],row[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(data[782])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORIGINAL EXAMPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get ORES Scores\n",
    "\n",
    "Below is an example of how to make a request through the ORES system in Python to find out the current quality of the article on [Aaron Halfaker](https://en.wikipedia.org/wiki/Aaron_Halfaker) (the person who created ORES):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually use the following link for documentation:  \n",
    "https://ores.wikimedia.org/v3/#!/scoring/get_v3_scores_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "endpoint = 'https://ores.wikimedia.org/v3/scores/{project}/{revid}/{model}'\n",
    "endpoint = 'https://ores.wikimedia.org/v3/scores/{project}/?models={model}&revids={revids}'\n",
    "headers = {'User-Agent' : 'https://github.com/your_github_username', 'From' : 'your_uw_email@uw.edu'}\n",
    "\n",
    "params = {'project' : 'enwiki',\n",
    "          'revids' : '797882120|797882121',\n",
    "          'model' : 'wp10'\n",
    "          }\n",
    "\n",
    "api_call = requests.get(endpoint.format(**params))\n",
    "response = api_call.json()\n",
    "print(json.dumps(response, indent=4, sort_keys=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the other data is just a matter of reading CSV files in! (and for the R programmers - we'll have an R example up as soon as the Hub supports the language)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## getting the data from the CSV files\n",
    "import csv\n",
    "\n",
    "data = []\n",
    "with open('page_data.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        data.append([row[0],row[1],row[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(data[782])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
